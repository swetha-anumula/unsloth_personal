{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0341060484945377,
  "eval_steps": 500,
  "global_step": 64,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005329070077271516,
      "grad_norm": 0.5154778361320496,
      "learning_rate": 0.0,
      "loss": 2.5241,
      "step": 1
    },
    {
      "epoch": 0.0010658140154543032,
      "grad_norm": 0.5039351582527161,
      "learning_rate": 2e-05,
      "loss": 2.8242,
      "step": 2
    },
    {
      "epoch": 0.0015987210231814548,
      "grad_norm": 0.5522323846817017,
      "learning_rate": 4e-05,
      "loss": 2.4462,
      "step": 3
    },
    {
      "epoch": 0.0021316280309086064,
      "grad_norm": 0.5963950753211975,
      "learning_rate": 6e-05,
      "loss": 2.4518,
      "step": 4
    },
    {
      "epoch": 0.002664535038635758,
      "grad_norm": 0.5750240683555603,
      "learning_rate": 8e-05,
      "loss": 2.5519,
      "step": 5
    },
    {
      "epoch": 0.0031974420463629096,
      "grad_norm": 0.5986661911010742,
      "learning_rate": 0.0001,
      "loss": 2.2179,
      "step": 6
    },
    {
      "epoch": 0.0037303490540900614,
      "grad_norm": 0.5589550137519836,
      "learning_rate": 0.00012,
      "loss": 2.2886,
      "step": 7
    },
    {
      "epoch": 0.004263256061817213,
      "grad_norm": 0.6286293268203735,
      "learning_rate": 0.00014,
      "loss": 2.625,
      "step": 8
    },
    {
      "epoch": 0.004796163069544364,
      "grad_norm": 0.5794445872306824,
      "learning_rate": 0.00016,
      "loss": 2.1729,
      "step": 9
    },
    {
      "epoch": 0.005329070077271516,
      "grad_norm": 0.67354816198349,
      "learning_rate": 0.00018,
      "loss": 2.6209,
      "step": 10
    },
    {
      "epoch": 0.005861977084998668,
      "grad_norm": 0.6101182699203491,
      "learning_rate": 0.0002,
      "loss": 1.8992,
      "step": 11
    },
    {
      "epoch": 0.006394884092725819,
      "grad_norm": 0.8110564947128296,
      "learning_rate": 0.0001962962962962963,
      "loss": 2.8101,
      "step": 12
    },
    {
      "epoch": 0.006927791100452971,
      "grad_norm": 0.7414931654930115,
      "learning_rate": 0.0001925925925925926,
      "loss": 2.4795,
      "step": 13
    },
    {
      "epoch": 0.007460698108180123,
      "grad_norm": 0.7064914703369141,
      "learning_rate": 0.00018888888888888888,
      "loss": 2.8896,
      "step": 14
    },
    {
      "epoch": 0.007993605115907274,
      "grad_norm": 1.153287410736084,
      "learning_rate": 0.0001851851851851852,
      "loss": 2.4754,
      "step": 15
    },
    {
      "epoch": 0.008526512123634426,
      "grad_norm": 1.0763506889343262,
      "learning_rate": 0.0001814814814814815,
      "loss": 2.9756,
      "step": 16
    },
    {
      "epoch": 0.009059419131361577,
      "grad_norm": 1.036068081855774,
      "learning_rate": 0.00017777777777777779,
      "loss": 2.1955,
      "step": 17
    },
    {
      "epoch": 0.009592326139088728,
      "grad_norm": 1.0380319356918335,
      "learning_rate": 0.00017407407407407408,
      "loss": 2.407,
      "step": 18
    },
    {
      "epoch": 0.010125233146815881,
      "grad_norm": 1.2975519895553589,
      "learning_rate": 0.00017037037037037037,
      "loss": 2.4115,
      "step": 19
    },
    {
      "epoch": 0.010658140154543033,
      "grad_norm": 1.3271452188491821,
      "learning_rate": 0.0001666666666666667,
      "loss": 2.8477,
      "step": 20
    },
    {
      "epoch": 0.011191047162270184,
      "grad_norm": 1.780899167060852,
      "learning_rate": 0.00016296296296296295,
      "loss": 2.7766,
      "step": 21
    },
    {
      "epoch": 0.011723954169997336,
      "grad_norm": 1.2983837127685547,
      "learning_rate": 0.00015925925925925927,
      "loss": 2.3446,
      "step": 22
    },
    {
      "epoch": 0.012256861177724487,
      "grad_norm": 1.8509020805358887,
      "learning_rate": 0.00015555555555555556,
      "loss": 2.184,
      "step": 23
    },
    {
      "epoch": 0.012789768185451638,
      "grad_norm": 2.8401474952697754,
      "learning_rate": 0.00015185185185185185,
      "loss": 2.8664,
      "step": 24
    },
    {
      "epoch": 0.01332267519317879,
      "grad_norm": 2.2047367095947266,
      "learning_rate": 0.00014814814814814815,
      "loss": 2.4707,
      "step": 25
    },
    {
      "epoch": 0.013855582200905941,
      "grad_norm": 2.986483335494995,
      "learning_rate": 0.00014444444444444444,
      "loss": 2.2989,
      "step": 26
    },
    {
      "epoch": 0.014388489208633094,
      "grad_norm": 2.1224443912506104,
      "learning_rate": 0.00014074074074074076,
      "loss": 2.4953,
      "step": 27
    },
    {
      "epoch": 0.014921396216360246,
      "grad_norm": 2.642432689666748,
      "learning_rate": 0.00013703703703703705,
      "loss": 2.3824,
      "step": 28
    },
    {
      "epoch": 0.015454303224087397,
      "grad_norm": 1.6552164554595947,
      "learning_rate": 0.00013333333333333334,
      "loss": 2.4717,
      "step": 29
    },
    {
      "epoch": 0.01598721023181455,
      "grad_norm": 3.5306363105773926,
      "learning_rate": 0.00012962962962962963,
      "loss": 2.649,
      "step": 30
    },
    {
      "epoch": 0.0165201172395417,
      "grad_norm": 2.9758591651916504,
      "learning_rate": 0.00012592592592592592,
      "loss": 2.5927,
      "step": 31
    },
    {
      "epoch": 0.01705302424726885,
      "grad_norm": 4.278465270996094,
      "learning_rate": 0.00012222222222222224,
      "loss": 2.7097,
      "step": 32
    },
    {
      "epoch": 0.017585931254996003,
      "grad_norm": 3.1150922775268555,
      "learning_rate": 0.00011851851851851852,
      "loss": 1.0621,
      "step": 33
    },
    {
      "epoch": 0.018118838262723154,
      "grad_norm": 6.474396228790283,
      "learning_rate": 0.00011481481481481482,
      "loss": 2.3096,
      "step": 34
    },
    {
      "epoch": 0.018651745270450305,
      "grad_norm": 2.3979249000549316,
      "learning_rate": 0.00011111111111111112,
      "loss": 2.1393,
      "step": 35
    },
    {
      "epoch": 0.019184652278177457,
      "grad_norm": 6.658825397491455,
      "learning_rate": 0.00010740740740740742,
      "loss": 2.5874,
      "step": 36
    },
    {
      "epoch": 0.019717559285904608,
      "grad_norm": 5.945788383483887,
      "learning_rate": 0.0001037037037037037,
      "loss": 2.3417,
      "step": 37
    },
    {
      "epoch": 0.020250466293631763,
      "grad_norm": 5.913701057434082,
      "learning_rate": 0.0001,
      "loss": 2.3022,
      "step": 38
    },
    {
      "epoch": 0.020783373301358914,
      "grad_norm": 2.336442232131958,
      "learning_rate": 9.62962962962963e-05,
      "loss": 2.4706,
      "step": 39
    },
    {
      "epoch": 0.021316280309086066,
      "grad_norm": 1.9722216129302979,
      "learning_rate": 9.25925925925926e-05,
      "loss": 2.0355,
      "step": 40
    },
    {
      "epoch": 0.021849187316813217,
      "grad_norm": 1.9484267234802246,
      "learning_rate": 8.888888888888889e-05,
      "loss": 2.2806,
      "step": 41
    },
    {
      "epoch": 0.02238209432454037,
      "grad_norm": 2.8032548427581787,
      "learning_rate": 8.518518518518518e-05,
      "loss": 3.0456,
      "step": 42
    },
    {
      "epoch": 0.02291500133226752,
      "grad_norm": 1.9873721599578857,
      "learning_rate": 8.148148148148148e-05,
      "loss": 2.2124,
      "step": 43
    },
    {
      "epoch": 0.02344790833999467,
      "grad_norm": 1.9540669918060303,
      "learning_rate": 7.777777777777778e-05,
      "loss": 2.6656,
      "step": 44
    },
    {
      "epoch": 0.023980815347721823,
      "grad_norm": 2.4111130237579346,
      "learning_rate": 7.407407407407407e-05,
      "loss": 2.4364,
      "step": 45
    },
    {
      "epoch": 0.024513722355448974,
      "grad_norm": 2.444129467010498,
      "learning_rate": 7.037037037037038e-05,
      "loss": 2.5283,
      "step": 46
    },
    {
      "epoch": 0.025046629363176125,
      "grad_norm": 2.21136474609375,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.2428,
      "step": 47
    },
    {
      "epoch": 0.025579536370903277,
      "grad_norm": 1.4461854696273804,
      "learning_rate": 6.296296296296296e-05,
      "loss": 2.0919,
      "step": 48
    },
    {
      "epoch": 0.026112443378630428,
      "grad_norm": 1.8289953470230103,
      "learning_rate": 5.925925925925926e-05,
      "loss": 2.3969,
      "step": 49
    },
    {
      "epoch": 0.02664535038635758,
      "grad_norm": 2.007878065109253,
      "learning_rate": 5.555555555555556e-05,
      "loss": 2.5254,
      "step": 50
    },
    {
      "epoch": 0.02717825739408473,
      "grad_norm": 1.6425549983978271,
      "learning_rate": 5.185185185185185e-05,
      "loss": 2.3407,
      "step": 51
    },
    {
      "epoch": 0.027711164401811882,
      "grad_norm": 2.330047130584717,
      "learning_rate": 4.814814814814815e-05,
      "loss": 2.3796,
      "step": 52
    },
    {
      "epoch": 0.028244071409539037,
      "grad_norm": 2.379150152206421,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 2.8098,
      "step": 53
    },
    {
      "epoch": 0.02877697841726619,
      "grad_norm": 2.62916898727417,
      "learning_rate": 4.074074074074074e-05,
      "loss": 2.269,
      "step": 54
    },
    {
      "epoch": 0.02930988542499334,
      "grad_norm": 3.249363899230957,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 2.0472,
      "step": 55
    },
    {
      "epoch": 0.02984279243272049,
      "grad_norm": 2.02555251121521,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.1451,
      "step": 56
    },
    {
      "epoch": 0.030375699440447643,
      "grad_norm": 1.9272674322128296,
      "learning_rate": 2.962962962962963e-05,
      "loss": 2.0569,
      "step": 57
    },
    {
      "epoch": 0.030908606448174794,
      "grad_norm": 2.2581307888031006,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 2.3809,
      "step": 58
    },
    {
      "epoch": 0.031441513455901945,
      "grad_norm": 1.6588093042373657,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 2.2683,
      "step": 59
    },
    {
      "epoch": 0.0319744204636291,
      "grad_norm": 2.422508478164673,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 2.4657,
      "step": 60
    },
    {
      "epoch": 0.03250732747135625,
      "grad_norm": 1.6426091194152832,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 2.1141,
      "step": 61
    },
    {
      "epoch": 0.0330402344790834,
      "grad_norm": 1.9679582118988037,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 2.5741,
      "step": 62
    },
    {
      "epoch": 0.03357314148681055,
      "grad_norm": 2.4317593574523926,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 2.823,
      "step": 63
    },
    {
      "epoch": 0.0341060484945377,
      "grad_norm": 2.4780914783477783,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 2.2435,
      "step": 64
    }
  ],
  "logging_steps": 1,
  "max_steps": 64,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 653732507320320.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
